{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Assignment7_new.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lL5ACbst-mTR",
        "colab": {}
      },
      "source": [
        "grading = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SwsaLxhGYpE4",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LTrezgijY5-2",
        "colab": {}
      },
      "source": [
        "test_df = pd.read_csv(\"a7_test.csv\")\n",
        "if not grading:\n",
        "  test_df\n",
        "\n",
        "train_df = pd.read_csv(\"a7_train.csv\")\n",
        "if not grading:\n",
        "  train_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VrRat9noZAvN",
        "colab": {}
      },
      "source": [
        "def calculate_priors(df):\n",
        "  \"\"\"\n",
        "  TASK 1a)\n",
        "\n",
        "  Calculates the probability of given classes, based on how often they appear\n",
        "  in a DataFrame.\n",
        "\n",
        "  You can assume that the class values for each row are in a column called\n",
        "  \"class\".\n",
        "\n",
        "  !!! Your function must work for any number of classes, so don't assume that \n",
        "  the number of classes is always exactly 2! !!!\n",
        "  \n",
        "  Return a dictionary mapping the class to its probability.\n",
        "  Example: The class \"apple\" appears in two rows, the class \"banana\" in three.\n",
        "  The probability for the \"apple\" class is 2/5, the probability for banana is \n",
        "  3/5.\n",
        "  You have to return a dictionary:\n",
        "    {\"apple\": 0.4, \"banana\": 0.6}\n",
        "\n",
        "  HINT: You can use .value_counts() to count the values in a column.\n",
        "    You have to select the correct column first.\n",
        "  HINT: You can use .apply to modify values with the use of a function, such as\n",
        "    a lambda function.\n",
        "\n",
        "  :param df: A pandas.DataFrame containing a column \"class\" and a column \"text\". \n",
        "  :return: A dictionary mapping the class to its probability.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  prior = df['class'].value_counts()\n",
        "  \n",
        "  prior = prior.apply(lambda x: x / prior.sum())\n",
        "\n",
        "  return prior.to_dict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0JoQGJsqNjKl",
        "outputId": "7d2636b7-cf81-4f28-a584-dfe32639844b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "############### EXTRA OUTPUT / TESTING ##################\n",
        "if not grading:\n",
        "  d = {\"class\": [\"apple\" if x < 3000 else \"banana\" for x in range(10000)], \"text\": [\"FOO!\" for x in range(10000)]}\n",
        "  test = pd.DataFrame(d)\n",
        "  p = calculate_priors(test)\n",
        "  # correct output: {'banana': 0.7, 'apple': 0.3}\n",
        "  print(p)\n",
        "  p = calculate_priors(train_df)\n",
        "  # correct output: {'pos': 0.5, 'neg': 0.5}\n",
        "  print(p)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'banana': 0.7, 'apple': 0.3}\n",
            "{'neg': 0.5, 'pos': 0.5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jk6OM3gPZmnB",
        "colab": {}
      },
      "source": [
        "if not grading:\n",
        "  priors = calculate_priors(train_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gWkmctLrZOr7",
        "colab": {}
      },
      "source": [
        "def calculate_class_term_frequency(df, classes):\n",
        "  \"\"\"\n",
        "  TASK 1b)\n",
        "\n",
        "  For each, calculates the frequency of terms appearing in texts of this class.\n",
        "\n",
        "  In the DataFrame, each row has an entry for \"text\" and one for \"class\".\n",
        "  The values in the \"text\" column are strings (= actual texts).\n",
        "  Your task is to calculate the frequency (= how often a word appears) for each\n",
        "  of the classes.\n",
        "\n",
        "  Example: \n",
        "    There are two texts with the \"apple\" class. \n",
        "    There are three texts with the \"banana\" class.\n",
        "    The word \"fruit\" appears five times in the texts of the \"apple\" class, and \n",
        "    two times for texts of the \"banana\" class.\n",
        "    The total frequency of the word \"fruit\" is 7. The frequency for the \"apple\"\n",
        "    class is 5. The frequency for the \"banana\" class is 2.\n",
        "\n",
        "  To get the individual terms you can use .split(\" \") (splitting by whitespace).\n",
        "  Convert the terms to lowercase.\n",
        "\n",
        "  HINT: You can iterate over the DataFrame rows with .iterrows():\n",
        "  https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iterrows.html\n",
        "\n",
        "  :param df: A pandas.DataFrame containing a column \"class\" and a column \"text\".\n",
        "  :param classes: A list of strings containing the possible values for the \"class\" column.\n",
        "  :return: A set containing all the terms (list of string) and a dictionary containing\n",
        "  the frequency of terms appearing in texts of each class in the following format:\n",
        "    {class(string): {term(string): frequency(int), ...}, ...}\n",
        "  \n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "  terms = set()\n",
        "  for index, row in df.iterrows():\n",
        "    for word in row['text'].split(\" \"):\n",
        "      terms.add(word.lower())\n",
        "\n",
        "  dct = dict()\n",
        "  for class_ in classes:\n",
        "    dct[class_] = defaultdict(int)\n",
        "    for index, row in df.iterrows():\n",
        "      if row['class'] == class_:\n",
        "        for term in row['text'].lower().split(\" \"):\n",
        "          dct[class_][term] += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  return terms, dct"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FNRgc2cMhSnE",
        "outputId": "004e75d2-e40b-42c3-b984-01cbd82eebcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "############### EXTRA OUTPUT / TESTING ##################\n",
        "if not grading:\n",
        "  terms, freqs_per_class = calculate_class_term_frequency(train_df, [\"pos\", \"neg\"])\n",
        "  # correct output: 252192\n",
        "  print(len(terms))\n",
        "  # correct output: 5335\n",
        "  print(freqs_per_class[\"neg\"][\"bad\"])\n",
        "  # correct output: 10\n",
        "  print(freqs_per_class[\"pos\"][\"rainbow\"])"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "252192\n",
            "5335\n",
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x0Hn1uHxZsed",
        "colab": {}
      },
      "source": [
        "if not grading:\n",
        "  classes = priors.keys()\n",
        "  terms, freqs_per_class = calculate_class_term_frequency(train_df, classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aYjtusnMZU0o"
      },
      "source": [
        "**Conditional Probabilities (with add-1 smoothing)**: $P(w|c) = \\frac{count(w, c) + 1}{count(c) + |V|}$\n",
        "\n",
        "Example: $P(fruit|apple) = 0.3$\n",
        "\n",
        "$count(w, c)$ = How often word $w$ appears in documents of class $c$.\n",
        "\n",
        "$count(c)$ = How many words appear in documents of class $c$.\n",
        "\n",
        "$|V|$ = How many different words exist in the dataset.\n",
        "\n",
        "**Prior**: $P_{prior}(c) = \\frac{N_c}{N}$, where $N$ is the total number of documents and $N_c$ is the number of documents with class $c$\n",
        "\n",
        "**Choosing a class for document $d$**: $argmax_{c \\in C} P(c|d)$ with $P(c|d) = P_{prior}(c) * \\prod_{w \\in d} P(w|c)$\n",
        "\n",
        "You can also refer to the lecture slides where you can find an example calculation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G82uFZKJZWA-",
        "outputId": "73d1eccf-67ed-4c66-92ba-8d3559db8038",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "if not grading:\n",
        "  example_dct = {\"fruit\": {\"apple\": 0.3, \"banana\": 0.4}, \"tree\": {\"apple\": 0.5, \"banana\": 0.1}}\n",
        "  print(example_dct[\"fruit\"])  # prints the inner dictionary\n",
        "  # prints the probability of \"fruit\" appearing in a document if the document has the class \"apple\"\n",
        "  print(example_dct[\"fruit\"][\"apple\"])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'apple': 0.3, 'banana': 0.4}\n",
            "0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Jtz2IcomZYna",
        "colab": {}
      },
      "source": [
        "def calculate_class_term_probs(terms, freqs_per_class):\n",
        "  \"\"\"\n",
        "  TASK 1c)\n",
        "\n",
        "  Calculates the probability that a term appears in documents of a given class \n",
        "  and returns a nested dictionary containing the probabilities.\n",
        "\n",
        "  Use Laplace (add-1) smoothing.\n",
        "  Return a nested dictionary. The outer dictionary needs map terms to the \n",
        "  inner dictionaries. The inner dictionary maps class labels to probabilities.\n",
        "\n",
        "  -> If the dictionary is accessed with a non-existing key but an existing class\n",
        "  it should return the value 1/|V| ! <--\n",
        "\n",
        "  Example:\n",
        "    dct = {\"fruit\": {\"apple\": 0.3, \"banana\": 0.4}, \"tree\": {\"apple\": 0.5, \"banana\": 0.1}}\n",
        "    foo = dct[\"rock\"][\"apple\"]\n",
        "    foo == 1/2\n",
        "    Two terms exist in total (fruit and tree) so the total vocabulary size is 2.\n",
        "\n",
        "  HINT: Use defaultdict(): https://docs.python.org/3.6/library/collections.html#collections.defaultdict\n",
        "\n",
        "  :param terms: A list of strings for classes to calculate the frequencies.\n",
        "  :param freqs_per_class: A dictionary containing the frequency of terms appearing in texts of each class.\n",
        "  :return: A nested dictionary contains the terms and classes with the term's appearance probability.\n",
        "  \n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  total_different_words = len(terms)\n",
        "\n",
        "  classes = set()\n",
        "  for class_ in freqs_per_class.keys():\n",
        "    classes.add(class_)\n",
        "\n",
        "  words_per_class = dict()\n",
        "  for class_ in classes:\n",
        "    words_per_class[class_] = sum(freqs_per_class[class_].values())\n",
        "    \n",
        "  #dct = defaultdict(lambda: defaultdict(lambda: 1/total_different_words))\n",
        "  a = 1/total_different_words\n",
        "  dct = defaultdict(lambda: {c: a for c in classes})\n",
        "  \n",
        "  for term in terms:\n",
        "    dct[term] = dict()\n",
        "    for class_ in classes:\n",
        "      absolute_frequency = freqs_per_class[class_][term]\n",
        "      words_in_class = words_per_class[class_]\n",
        "\n",
        "      dct[term][class_] = (absolute_frequency + 1) / (words_in_class + total_different_words)\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "  return dct"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oeFX353_hFgZ",
        "outputId": "fa5eeb5f-57b8-4eb3-d405-24ead9aa5d8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "############### EXTRA OUTPUT / TESTING ##################\n",
        "if not grading:\n",
        "  cond_probs = calculate_class_term_probs(terms, freqs_per_class)\n",
        "  # correct: 0.00013765662333908876\n",
        "  print(cond_probs[\"happy\"][\"pos\"])\n",
        "  # correct: 8.604442314225311e-05\n",
        "  print(cond_probs[\"happy\"][\"neg\"])\n",
        "  # correct: 3.965232838472275e-06\n",
        "  # NOTE: this is a word that does not exist in the data. therefore the value\n",
        "  # becomes 1/len(terms) - this is what the formula above for laplace smoothing\n",
        "  # tells us \n",
        "  print(cond_probs[\"pneumonoultramicroscopicsilicovolcanoconiosis\"][\"pos\"])\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.00013765662333908876\n",
            "8.604442314225311e-05\n",
            "3.965232838472275e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tLHXVO6TaIfg",
        "colab": {}
      },
      "source": [
        "if not grading:\n",
        "  probs = calculate_class_term_probs(terms, freqs_per_class)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JRlVMNfrZcQl",
        "colab": {}
      },
      "source": [
        "def classify(text, priors, probs):\n",
        "  \"\"\"\n",
        "  TASK 1d)\n",
        "  This function should predict a class for a given text.\n",
        "  Use the priors you calculated in 1a).\n",
        "  Use the probabilities you calculated in 1c).\n",
        "\n",
        "  !!!!!!!\n",
        "  Instead of multiplying the probabilites together you can take the logarithm\n",
        "  of the probabilities and add them together. \n",
        "  !!!!!!!\n",
        "\n",
        "  Use the Naive Bayes classification to assign the class.\n",
        "  \n",
        "  :param text: A string contains the text to clasiffy.\n",
        "  :param priors: A dictionary mapping the class to its probability (From TASK 1a).\n",
        "  :param probs: A nested dictionary contains the terms and classes with\n",
        "  the term's appearance probability (From TASK 1c).\n",
        "  :return: A string for the class label predicted.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  classes = set()\n",
        "\n",
        "  for class_ in priors:\n",
        "    classes.add(class_)\n",
        "  \n",
        "  highestProb = []\n",
        "  for class_ in classes:\n",
        "    probability = 0\n",
        "    for word in text.lower().split(\" \"):\n",
        "      probability = probability + math.log(probs[word][class_])\n",
        "\n",
        "    prob = math.log(priors[class_]) + probability\n",
        "\n",
        "    if len(highestProb) == 0:\n",
        "      highestProb = [class_, prob]\n",
        "    elif prob > highestProb[1]:\n",
        "      highestProb = [class_, prob]\n",
        "    \n",
        "  return highestProb[0]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J09bPDOIg0CZ",
        "outputId": "0da19a73-74bc-4d98-cfc9-1c7dbaef7e18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "\n",
        "############### EXTRA OUTPUT / TESTING ##################\n",
        "if not grading:\n",
        "  t1 = \"Today is an awful day.\"\n",
        "  pred = classify(t1, priors, probs)\n",
        "  # neg\n",
        "  print(f\"'{t1}' is {pred}\")\n",
        "  t2 = \"Today is a nice day.\"\n",
        "  pred = classify(t2, priors, probs)\n",
        "  # pos\n",
        "  print(f\"'{t2}' is {pred}\")\n",
        "  t3 = \"pneumonoultramicroscopicsilicovolcanoconiosis\"\n",
        "  pred = classify(t3, priors, probs)\n",
        "  # pos\n",
        "  print(f\"'{t3}' is {pred}\")\n",
        "  t4 = \"This movie was very good and I liked it a lot.\"\n",
        "  pred = classify(t4, priors, probs)\n",
        "  # pos\n",
        "  print(f\"'{t4}' is {pred}\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Today is an awful day.' is neg\n",
            "'Today is a nice day.' is pos\n",
            "'pneumonoultramicroscopicsilicovolcanoconiosis' is pos\n",
            "'This movie was very good and I liked it a lot.' is pos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lMUwhG-5ZfG4",
        "colab": {}
      },
      "source": [
        "def evaluate(test, priors, probs):\n",
        "  \"\"\"\n",
        "  TASK 1e)\n",
        "  Evaluate your classification.\n",
        "\n",
        "  -> You may assume that the only classes are \"pos\" and \"neg\" ! <--\n",
        "\n",
        "  The function takes a DataFrame to evaluate the classifier from TASK 1d.\n",
        "  For each row in the test dataset, get the actual class from the \"class\" column\n",
        "  and the text from the \"text\" column.\n",
        "  Use your \"classify(text, priors, probs)\" function to get the predicted\n",
        "  class label for each text.\n",
        "\n",
        "  HINT: Count the TP, FP, FN for each of the two classes, for each prediction. \n",
        "\n",
        "  Calculate the following metrics PER CLASS:\n",
        "    - precision\n",
        "    - recall\n",
        "    - f1-measure\n",
        "\n",
        "  :param test: A pandas.DataFrame containing a column \"class\" and a column \"text\".\n",
        "  :param priors: A dictionary mapping the class to its probability (From TASK 1a).\n",
        "  :param probs: A nested dictionary contains the terms and classes with\n",
        "  the term's appearance probability (From TASK 1c).\n",
        "  :return: A list of tuples in the following format:\n",
        "    [(class(string), precision(float), recall(float), f1-measure(float)), ...]\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  results = list()\n",
        "\n",
        "  def perf_measure(y_actual, y_hat, positive, negative):\n",
        "    TP = 0\n",
        "    FP = 0\n",
        "    TN = 0\n",
        "    FN = 0\n",
        "\n",
        "    for i in range(len(y_hat)): \n",
        "        if y_actual[i]==y_hat[i]==positive:\n",
        "           TP += 1\n",
        "        if y_hat[i]==positive and y_actual[i]!=y_hat[i]:\n",
        "           FP += 1\n",
        "        if y_actual[i]==y_hat[i]==negative:\n",
        "           TN += 1\n",
        "        if y_hat[i]==negative and y_actual[i]!=y_hat[i]:\n",
        "           FN += 1\n",
        "\n",
        "    return(TP, FP, TN, FN)\n",
        "\n",
        "  real_value = list()\n",
        "  predictions = list()\n",
        "  for i, row in test.iterrows():\n",
        "      real_value.append(row['class'])\n",
        "      predictions.append(classify(row['text'], priors, probs))\n",
        "\n",
        "  #Wrong values (does it even make sense to calculate per class!!!!!!!)\n",
        "  tp, fp, tn, fn = perf_measure(real_value, predictions, 'pos', 'neg')\n",
        "  precision = tp / (tp + fp)\n",
        "  recall = tp / (tp + fn)\n",
        "  f1_measure = 2*(precision * recall)/(precision + recall)\n",
        "  results.append(('pos', precision, recall, f1_measure))\n",
        "\n",
        "  tp, fp, tn, fn = perf_measure(real_value, predictions, 'neg', 'pos')\n",
        "  precision = tp / (tp + fp)\n",
        "  recall = tp / (tp + fn)\n",
        "  f1_measure = 2*(precision * recall)/(precision + recall)\n",
        "  results.append(('neg', precision, recall, f1_measure))\n",
        "\n",
        "  return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FILR7HD_aPKX",
        "colab": {}
      },
      "source": [
        "if not grading:\n",
        "  results = evaluate(test_df, priors, probs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1P37lDUUgvFM",
        "outputId": "8a55771a-fb72-430f-b97f-4922634454c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "############### EXTRA OUTPUT / TESTING ##################\n",
        "if not grading:\n",
        "  results = evaluate(test_df, priors, probs)\n",
        "  for cls, p, r, f1 in results:\n",
        "    print(cls, p, r, f1)\n",
        "  # pos 0.8594459784448205 0.77192 0.8133350191764657\n",
        "  # neg 0.7930007986640528 0.87376 0.8314238952536825\n",
        "\n",
        "#Does not give right output???!!!"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos 0.8594459784448205 0.77192 0.8133350191764657\n",
            "neg 0.7930007986640528 0.87376 0.8314238952536825\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}