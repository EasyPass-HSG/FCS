{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4af1df28fedc5707576d0dd50c98728f.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKNaurvjTShp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grading = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9y7pkknmTW8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MC3gFWWTYNq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_questions():\n",
        "    \"\"\"\n",
        "    Task 1a)\n",
        "\n",
        "    Load the questions and their classes.\n",
        "\n",
        "    Complete the following steps:\n",
        "      - load the file \"train_5500.label.txt\" with \"ISO-8859-1\" encoding\n",
        "      - read the lines of the file\n",
        "      - consider only the lines that start with \"LOC:city\" or \"HUM:ind\"\n",
        "      - create a list in the following format:\n",
        "        [ (text, class), ... ]\n",
        "      - \"text\" is the content of the line AFTER the label\n",
        "         LOC:city What is the name of the city that Maurizio Pellegrin lives in ?\n",
        "        For this line the content would be: \"What is the name of the city that Maurizio Pellegrin lives in ?\"\n",
        "      - class is based on the label: \"CITY\" for \"LOC:city\" and \"HUMAN\" for \"HUM:ind\"\n",
        "        For the example above this would be \"CITY\"\n",
        "      - keep only the first 128 lines with class \"HUMAN\" (labelled as \"HUM:ind\")\n",
        "      - keep all samples for \"CITY\"\n",
        "      - return the list in the format described above\n",
        "    \n",
        "    INPUT:\n",
        "      - no input\n",
        "    OUTPUT:\n",
        "      - texts: a list of tuples\n",
        "        Each tuple should represent ONE (1) line from the text file \"train_5500.label.txt\"\n",
        "        The first element in the tuple is the the question part of the line.\n",
        "        That means everything after the label.\n",
        "        Example:\n",
        "          Consider this line from the text file:\n",
        "            HUM:ind Name the scar-faced bounty hunter of The Old West .\n",
        "          To your output list you have to add this tuple:\n",
        "            (\"Name the scar-faced bounty hunter of The Old West .\", \"HUM:ind\")\n",
        "                          ^ the text of the question                    ^ class\n",
        "    \"\"\"\n",
        "    texts = []\n",
        "\n",
        "    f = open(\"train_5500.label.txt\", mode=\"r\", encoding=\"ISO-8859-1\")\n",
        "    lines = f.readlines()\n",
        "\n",
        "    hum_count = 0\n",
        "\n",
        "    for line in lines:\n",
        "      line = line.split(\" \")\n",
        "      if \"LOC:city\" == line[0]:\n",
        "        text = \" \".join(line[1:])\n",
        "        tpl = (text, \"CITY\")\n",
        "        texts.append(tpl)\n",
        "      elif \"HUM:ind\" == line[0] and hum_count < 128:\n",
        "        text = \" \".join(line[1:])\n",
        "        tpl = (str(text), \"HUMAN\")\n",
        "        hum_count += 1\n",
        "        texts.append(tpl)\n",
        "      \n",
        "    f.close()\n",
        "\n",
        "    ### YOUR CODE HERE ###\n",
        "    return texts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YKQ4hcGO0_r",
        "colab_type": "code",
        "outputId": "5081ca73-394f-4ac3-a91b-32520ee991d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if not grading:\n",
        "    # you need to import this before you can use it\n",
        "    from collections import Counter\n",
        "    # this is the function you must implement in task 1a)\n",
        "    texts = load_questions()\n",
        "    # this counts how often the second element (accessed with x[1]) appears\n",
        "    # this means it counts how often each class appears in your dataset\n",
        "    count = Counter(x[1] for x in texts)\n",
        "    # the \"CITY\" class should appear 129 times, the \"HUMAN\" class should appear 128 times\n",
        "    print(count)\n",
        "    # Counter({'CITY': 129, 'HUMAN': 128})"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({'CITY': 129, 'HUMAN': 128})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2Ohb-E6Tgm5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_dataframe_q(texts):\n",
        "    \"\"\"\n",
        "    Task 1b)\n",
        "\n",
        "    Process the texts and create a DataFrame you can use for training a classifier.\n",
        "\n",
        "    Complete the following steps:\n",
        "      - load the spacy model \"en_core_web_sm\"\n",
        "      - for each text, create a spacy doc object\n",
        "      - for each token in this doc object, do:\n",
        "        - if the token begins an entity, keep the entity type\n",
        "        - elif the token is outside an entity, and not a stopword, keep token.text\n",
        "        - if the token is a stopword, continue\n",
        "        - keep token.text\n",
        "          (if the token is outside an entity then this means you keep token.text twice)\n",
        "        - you can find everything you need here: https://spacy.io/api/token\n",
        "        - create a dictionary for the current document in this format:\n",
        "          {\n",
        "            \"text\": list of the tokens joined with whitespace (use \" \".join(tokens)),\n",
        "            \"class\": the class of the text,\n",
        "          }\n",
        "        - store this dictionary, for each document, in a list\n",
        "        - create a new dataframe where each row is represented by one of the\n",
        "         document dictionaries\n",
        "        - return this dataframe\n",
        "\n",
        "    vvvvvvvvvvvvvvvvvvvv\n",
        "    To check whether or not a token is inside an entity, you can use this code.\n",
        "    You need to put in a for-loop. This for-loop must iterate over each\n",
        "    token in a text.\n",
        "\n",
        "      t = []\n",
        "      if token.ent_iob == 3:\n",
        "          t.append(token.ent_type_)\n",
        "      elif token.ent_iob == 2 and not token.is_stop:\n",
        "          t.append(token.text)\n",
        "      # if token is not a stopword, then use \"continue\" here\n",
        "      # if the token is NOT a stopword, add it to your list \"t\"\n",
        "\n",
        "    \"token\" comes from a spaCy Document object. Refer to the spaCy documentation\n",
        "    or the lecture slides to see how to process the tokens in a text with\n",
        "    spaCy.\n",
        "    You also need to check if a token is a stopword. If it is a stopword, use\n",
        "    the \"continue\" keyword to go to the next token in your for-loop.\n",
        "    After you have processed all tokens for the current text, combine them \n",
        "    together from a list of strings (your \"t\" list) to a string.\n",
        "    vvvvvvvvvvvvvvvvvvvv\n",
        "\n",
        "    INPUT:\n",
        "      - texts: The list of texts that you get from your task 1a) function.\n",
        "        You need to pre-process the texts. In this case, that means that you have to\n",
        "        remove stopwords.\n",
        "    OUTPUT:\n",
        "      - df: A DataFrame, where each row represents one text / question from the \"texts\"\n",
        "        list that you get as a function argument.\n",
        "    PSEUDOCODE:\n",
        "      This is not actual code but it describes what your function will have to do.\n",
        "      If you translate this \"fake\" code into Python then your function is correct.\n",
        "    ```\n",
        "    rows = []\n",
        "    nlp = load spacy model\n",
        "    for text, class in texts:\n",
        "        doc = make spaCy document object out of \"text\"\n",
        "        t = []\n",
        "        for token in doc:\n",
        "            if token.ent_iob == 3:\n",
        "                t.append(token.ent_type_)\n",
        "            elif token.ent_iob == 2 and not token.is_stop:\n",
        "                t.append(token.text)\n",
        "            if token is a stopword:\n",
        "                continue\n",
        "            append \"token.text\" to the list \"t\"\n",
        "        t = \"Â \".join(t)\n",
        "        text_information = dictionary that has the keys \"text\" and \"class\" that maps to the joined together string (t) and the class of the text (from the for loop)\n",
        "        append \"text_information\" to the list \"rows\"\n",
        "    df = a DataFrame that contains the content of the \"rows\" list as actual DataFrame rows\n",
        "    return df\n",
        "    ```\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "\n",
        "  \n",
        "\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    for text in texts:\n",
        "      #Why text[0] (what is text???)\n",
        "      doc = nlp(text[0])\n",
        "      tokens = []\n",
        "      dct = {}\n",
        "\n",
        "      #Recheck if all if conditions are right according to what they write in comments!Ã¨!!!!!!\n",
        "      for token in doc:\n",
        "        if token.ent_iob == 3:\n",
        "          tokens.append(token.ent_type_)\n",
        "        elif token.ent_iob == 2 and not token.is_stop:\n",
        "          tokens.append(token.text)\n",
        "        elif token.is_stop:\n",
        "          continue\n",
        "        tokens.append(token.text)\n",
        "\n",
        "      dct[\"text\"] = \" \".join(tokens)\n",
        "      dct[\"class\"] = text[1]\n",
        "      rows.append(dct)\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "    ### YOUR CODE HERE ###\n",
        "    df = pd.DataFrame(rows)\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XENeY7BDSj9M",
        "colab_type": "code",
        "outputId": "881df4dd-b655-43a8-f8ce-c1160a36834c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "if not grading:\n",
        "    texts = load_questions()\n",
        "    df = build_dataframe_q(texts)\n",
        "    print(df.iloc[10])\n",
        "    # text        city city oldest oldest relationship relations...\n",
        "    # class                                                    CITY\n",
        "    # Name: 10, dtype: object\n",
        "    print(\"----\")\n",
        "    print(df.iloc[100])\n",
        "    # text        FAC McCarren Airport located located city city...\n",
        "    # class                                                    CITY\n",
        "    # Name: 100, dtype: object\n",
        "    print(\"----\")\n",
        "    print(df.iloc[123])\n",
        "    # text        graced graced airwaves airwaves pearls pearls ...\n",
        "    # class                                                   HUMAN\n",
        "    # Name: 123, dtype: object"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text     city city oldest oldest relationship relations...\n",
            "class                                                 CITY\n",
            "Name: 10, dtype: object\n",
            "----\n",
            "text     FAC McCarren Airport located located city city...\n",
            "class                                                 CITY\n",
            "Name: 100, dtype: object\n",
            "----\n",
            "text     graced graced airwaves airwaves pearls pearls ...\n",
            "class                                                HUMAN\n",
            "Name: 123, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJaTDptlTmsF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_train_test_set(df, train_size, test_size, random_state=42):\n",
        "    \"\"\"\n",
        "    Task 1c)\n",
        "\n",
        "    Split a DataFrame into training and test set.\n",
        "    Use a method from scikit-learn to accomplish this.\n",
        "\n",
        "    !!! You need to import this scikit-learn method! !!!\n",
        "\n",
        "    vvvvvvvvvvvvvvvvvvvv\n",
        "    You can find the correct function in lecture slides or with Google!\n",
        "    vvvvvvvvvvvvvvvvvvvv\n",
        "\n",
        "\n",
        "    Specify the train_size, test_size and random_state of this scikit-learn method\n",
        "    to use the values passed into this function.\n",
        "\n",
        "    INPUT:\n",
        "      - df: The DataFrame that was created by your \"build_dataframe_q\" function.\n",
        "      - train_size: The size of your training set, between 0 and 1. 0.6 for example\n",
        "        means that you use 60% of your data for the training set.\n",
        "      - test_size: The size of your test set, between 0 and 1. 0.4 for example\n",
        "        means that you use 60% of your data for the test set.\n",
        "      - random_state: This is an optional parameter that you need to use in the \n",
        "        scikit-learn method that helps you split your data into training and test set.\n",
        "    OUTPUT:\n",
        "      - X_train: The texts that are going to be used for training.\n",
        "      - X_test: The texts that are going to be used for validation.\n",
        "      - y_train: The correct labels/classes of the texts in X_train.\n",
        "      - y_test: The correct labels/classes of the texts in X_test.\n",
        "    PSEUDOCODE:\n",
        "      Find and import the correct method from scikit-learn. The required method\n",
        "      lets you split your data into training and test set. You can find it in the\n",
        "      official scikit-learn documentation or in the lecture slides.\n",
        "    ```\n",
        "    from sklearn.model_selection import name_of_the_function\n",
        "    X = the \"text\" column of \"df\" (the first function argument)\n",
        "    y = the \"class\" column of \"df\"\n",
        "    X_train, X_test, y_train, y_test = name_of_the_function(X, y, \n",
        "        train_size=train_size, \n",
        "        test_size=test_size, \n",
        "        random_state=random_state)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(df['text'], df['class'], train_size=train_size, test_size=test_size, random_state=random_state)\n",
        "\n",
        "    ### YOUR CODE HERE ###\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BLuuqkpSm-G",
        "colab_type": "code",
        "outputId": "025e9d41-a804-42b4-dd39-37875faec475",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "if not grading:\n",
        "    # your \"create_train_test_set\" function is called here\n",
        "    texts = load_questions()\n",
        "    df = build_dataframe_q(texts)\n",
        "    X_train, X_test, y_train, y_test = create_train_test_set(df, 0.6, 0.4)\n",
        "    print(len(X_train))\n",
        "    print(len(X_test))\n",
        "    print(len(y_train))\n",
        "    print(len(y_test))\n",
        "    # 154\n",
        "    # 103\n",
        "    # 154\n",
        "    # 103"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "154\n",
            "103\n",
            "154\n",
            "103\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUyOMB63TubO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gridsearch(clf, parameters, X, y):\n",
        "    \"\"\"\n",
        "    Task 1d)\n",
        "\n",
        "    Perform a gridsearch for a given classifier, data and parameters.\n",
        "    Print the best parameters and also return them.\n",
        "\n",
        "    The parameters of this function are the same ones you can pass directly into\n",
        "    the scikit-learn GridSearchCV: \n",
        "    https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
        "\n",
        "    INPUT:\n",
        "      - clf: A classifier object. For example, this could be \"PassiveAggressiveClassifier()\"\n",
        "      - parameters: A dictionary containing the paramaters that you use in the gridsearch.\n",
        "        You have to read the documentation of the classifier to find what parameters exist for it!\n",
        "        EXAMPLE:\n",
        "          parameters = {\n",
        "            \"C\": [x/10 for x in range(0, 11)],\n",
        "            \"fit_intercept\": [True, False],\n",
        "            \"max_iter\": [x*1000 for x in range(1, 4)],\n",
        "            \"shuffle\": [False],\n",
        "            \"random_state\": [42],\n",
        "            \"tol\": [0.0001, 0.001, 0.01, 0.1],\n",
        "          }\n",
        "      - X: The training data\n",
        "      - y: The correct labels/classes of the training data\n",
        "    OUTPUT:\n",
        "      - clf.best_params_: The best parameters, as found by the gridsearch, as a dictionary.\n",
        "    PSEUDOCODE:\n",
        "    ```\n",
        "    clf = GridSearchCV(clf, parameters)\n",
        "    train \"clf\" by giving it training_data (function argument \"X\") and training data labels (function argument \"Y\") Training GridSearchCV works exactly like training a classifier!\n",
        "    print(clf.best_params_)   <-- these are the parameters that you have to use in task 2!\n",
        "    return clf.best_params_\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    clf.fit(X,y)\n",
        "    grid_search = GridSearchCV(estimator=clf, param_grid=parameters, cv=10)\n",
        "    grid_search = grid_search.fit(X, y)\n",
        "\n",
        "    best_param = dict(grid_search.best_params_)\n",
        "    print(best_param)\n",
        "    \n",
        "    return best_param"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cYSxSK1TyTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_classifier(true_labels, predictions):\n",
        "    \"\"\"\n",
        "    Task 1e)\n",
        "\n",
        "    true_labels = The class labels of your test set.\n",
        "    predictions = The predicted labels, predictions made by your classifier.\n",
        "\n",
        "    Evaluate how good the predictions of a classifier are.\n",
        "    \"true_labels\" are the correct class labels, \"predictions\" is what the\n",
        "    classifier predicted.\n",
        "\n",
        "    You have to call this method in your \"train_classifier\" method (task 1f)!\n",
        "\n",
        "    INPUT:\n",
        "      - true_labels: The correct classes/labels of your test set\n",
        "      - predictions: The predictions that your classifier has made\n",
        "    OUTPUT:\n",
        "      - accuracy: The accuracy classification score\n",
        "      - precision: The precision (weighted)\n",
        "      - recall: The recall (weighted)\n",
        "      - f1: The F1-measure (weighted)\n",
        "    PSEUDOCODE:\n",
        "    ```\n",
        "    accuracy = calculate accuracy using the correct method\n",
        "    precision = calculate precision using the correct method\n",
        "    recall = calculate recall using the correct method\n",
        "    f1 = calculate f1 using the correct method\n",
        "    return accuracy, precision, recall, f1\n",
        "    ```\n",
        "    EXTRA HINT:\n",
        "      The functions you need are already imported - look near the top of\n",
        "      this notebook file to find their names!\n",
        "    \n",
        "      --> Use \"weighted\" for the \"average\" argument in the evaluation methods! <--\n",
        "      If you don't know what this means, refer to the lecture slides or read the\n",
        "      documentation of the functions!\n",
        "    \"\"\"\n",
        "    accuracy = accuracy_score(y_true=true_labels, y_pred=predictions)\n",
        "  \n",
        "    precision = precision_score(y_true=true_labels, y_pred=predictions, average='weighted')\n",
        "  \n",
        "    recall = recall_score(y_true=true_labels, y_pred=predictions, average='weighted')\n",
        "    f1 = f1_score(y_true=true_labels, y_pred=predictions, average='weighted')\n",
        "\n",
        "\n",
        "    return accuracy, precision, recall, f1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYMo-RRGOLkn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_classifier(df, clf, do_gridsearch=False, parameters=None):\n",
        "    \"\"\"\n",
        "    Task 1f)\n",
        "\n",
        "    Train and evaluate a classifier.\n",
        "\n",
        "    - Split the data (df) into training and test set, using your\n",
        "      \"create_train_test_set\" method.\n",
        "      Use a 60:40 training:test split.\n",
        "    - create a vectorizer object. You can find vectorizers here:\n",
        "      https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text\n",
        "      Pick one that you think is suitable. If you are unsure, pick any one of \n",
        "      them and evaluate your classifier. Then replace the vectorizer (change\n",
        "      your code to use a different vectorizer and run the cell again) \n",
        "      and see how the results change.\n",
        "      The task should be solvable with any of the vectorizers, but some of them\n",
        "      are better suited for this task than others.\n",
        "    - transform your training data using the .fit_transform() method of\n",
        "      the vectorizer (read the scikit-learn documentation of your vectorizer\n",
        "      to see how to call this method)\n",
        "    - use the .fit method of the classifier to train your classifier\n",
        "      Example here:\n",
        "      https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.PassiveAggressiveClassifier.html#sklearn.linear_model.PassiveAggressiveClassifier.fit\n",
        "    - get the predictions for your test data using clf.predict()\n",
        "      HINT: There is something you need to do with your test data beforehand.\n",
        "            You had to do the same thing with your training data before you could\n",
        "            use the .fit method..\n",
        "    - calculate accuracy, precision, recall and f1-measure of your classifier\n",
        "      based on the predictions of your test set and the vectorizer\n",
        "      Use your \"evaluate_classifier\" method for this.\n",
        "      Return these values in the format (accuracy, precision, recall, f1, vectorizer)\n",
        "\n",
        "    INPUT:\n",
        "      - df: The DataFrame as it was created by your \"build_dataframe_q\" function\n",
        "      - clf: A classifier object. See for example task 2a) what it looks like\n",
        "      - do_gridsearch: An optional parameter that is set to \"False\" by default.\n",
        "        If it is set to \"True\" then you need to call your \"gridsearch\" method\n",
        "        and perform a gridsearch for the current classifier.\n",
        "      - parameters: An optional parameter. If \"do_gridsearch\" is set to \"True\"\n",
        "        then you need to pass this into your \"gridsearch\" function.\n",
        "    OUTPUT:\n",
        "      - accuracy: The accuracy of the classifier's performance\n",
        "      - precision: The precision of the classifier's performance\n",
        "      - recall: The recall of the classifier's performance\n",
        "      - f1: The F1-measure of your classifier's performance\n",
        "      - vectorizer: The vectorizer used\n",
        "    PSEUDOCODE:\n",
        "    ```\n",
        "    # create_train_test_set is the function you implemented for 1c)\n",
        "    X_train, X_test, y_train, y_test = create_train_test_set(df, 0.6, 0.4)\n",
        "    # remove the index, the following two lines ensure you have only text data\n",
        "    X_train = X_train[\"text\"]\n",
        "    X_test = X_test[\"text]\n",
        "    vectorizer = create a vectorizer. Pick any vectorizer that you like. Refer to the lecture slides to find which vectorizers exist. Experiment with different vectorizers by changing your code to use a different one.\n",
        "    X_train = vectorize \"X_train\" using \"vectorizer\". Refer to the lecture slides if you are unsure how to do this.\n",
        "    if do_gridsearch:\n",
        "        gridsearch(clf, parameters, X_train, y_train)\n",
        "    train your classifier with \"X_train\" and \"y_train\" If you are unsure how to do this, refer to the lecture slides.\n",
        "    X_test = vectorize \"X_test\" using \"vectorizer\". !! Ensure to use the correct method for transforming test data. !!\n",
        "    predictions = clf.predict(X_test)\n",
        "    # this uses your \"evaluate_classifier\" function that you did for the previous task\n",
        "    accuracy, precision, recall, f1 = evaluate_classifier(y_test, predictions)\n",
        "    return accuracy, precision, recall, f1, vectorizer\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    X_train, X_test, y_train, y_test = create_train_test_set(df, 0.6, 0.4)\n",
        "\n",
        "\n",
        "\n",
        "    #X_train = X_train[\"text\"]\n",
        "    #X_test = X_test[\"text\"]\n",
        "    X_train = X_train.to_list()\n",
        "    X_test = X_test.to_list()\n",
        "  \n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X_train = vectorizer.fit_transform(X_train)\n",
        "    if do_gridsearch==True:\n",
        "      best_param = gridsearch(clf, parameters, X_train, y_train)\n",
        "\n",
        "      #We could even set the best parameters here if we wanted to!\n",
        "      #clf.set_params(**best_param)\n",
        "  \n",
        "    clf.fit(X_train, y_train)\n",
        "  \n",
        "    y_test_predicted = clf.predict(vectorizer.transform(X_test))\n",
        "    accuracy, precision, recall, f1 = evaluate_classifier(y_test, y_test_predicted)\n",
        "\n",
        "    return accuracy, precision, recall, f1, vectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Md9ApVqnA9G",
        "colab_type": "text"
      },
      "source": [
        "# For task 2 all you have to do is find optimized parameters for the classifiers.\n",
        "For example in task 2a):\n",
        "\n",
        "\n",
        "```\n",
        "  clf = PassiveAggressiveClassifier(random_state=42)\n",
        "```\n",
        "\n",
        "\n",
        "You need to CHANGE THIS LINE. You have to provide optional parameters! For example:\n",
        "\n",
        "```\n",
        "  clf = PassiveAggressiveClassifier(random_state=42, C=3)\n",
        "```\n",
        "We know that \"PassiveAggressiveClassifier\" has this parameter \"C\" because of the documentation:\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.PassiveAggressiveClassifier.html\n",
        "\n",
        "You can find examples of how to use GridSearchCV in the lecture or here:\n",
        "\n",
        "https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u0NoK_hT94o",
        "colab_type": "code",
        "outputId": "c849ec2d-fc45-402f-9b03-efd19233f4ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "def build_pa(df):\n",
        "    \"\"\"\n",
        "    Task 2a)\n",
        "\n",
        "    https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.PassiveAggressiveClassifier.html\n",
        "    Train a PassiveAggressiveClassifier on the provided data.\n",
        "    Accuracy, precision, recall and f1-measure should all be above 96%.\n",
        "\n",
        "    Optimize the classifier parameters so the result of your \"train_classifier\" fucntion are all above 96%.\n",
        "\n",
        "    HINT: Perform a gridsearch to find optimized parameters to improve your classifier.\n",
        "    \n",
        "    :return: accuracy, precision, recall, f1, vectorizer\n",
        "    \"\"\"\n",
        "    #clf = PassiveAggressiveClassifier(random_state=42)\n",
        "    #Optimized classifier\n",
        "\n",
        "    # Instead of setting the parameters here we could also set them after the gridsearch directly.\n",
        "    clf = PassiveAggressiveClassifier(C=0.3 , fit_intercept=True , max_iter=1000 , shuffle=True , random_state=10 , tol=0.0001)\n",
        "\n",
        "\n",
        "    #Parameters for grid search\n",
        "    params = {\"C\": [x/10 for x in range(0, 11)],\n",
        "            \"fit_intercept\": [True, False],\n",
        "            \"max_iter\": [x*1000 for x in range(1, 4)],\n",
        "            \"shuffle\": [True, False],\n",
        "            \"random_state\": [42, 10],\n",
        "            \"tol\": [0.0001, 0.001, 0.01, 0.1]}\n",
        "\n",
        "    accuracy, precision, recall, f1, v = train_classifier(df, clf, do_gridsearch=False, parameters=params)\n",
        "    print(accuracy, precision, recall, f1)\n",
        "    pred = clf.predict(v.transform([\"Who is Ghandi?\"]))\n",
        "    print(pred)\n",
        "    return accuracy, precision, recall, f1, v\n",
        "\n",
        "if not grading:\n",
        "    q = load_questions()\n",
        "    df = build_dataframe_q(q)\n",
        "    accuracy, precision, recall, f1, v = build_pa(df)\n",
        "    print(accuracy, precision, recall, f1)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.970873786407767 0.97106781472858 0.970873786407767 0.9708792793287648\n",
            "['HUMAN']\n",
            "0.970873786407767 0.97106781472858 0.970873786407767 0.9708792793287648\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZAkJ4AyUDqe",
        "colab_type": "code",
        "outputId": "aa38ffd6-ccd6-4a50-d930-241c639dd3e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "def build_mnb(df):\n",
        "    \"\"\"\n",
        "    Task 2b)\n",
        "\n",
        "    https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html \n",
        "    Train an MultinomialNB classifier on the provided data.\n",
        "    Accuracy, precision, recall and f1-measure should all be above 79%.\n",
        "\n",
        "    Optimize the classifier parameters so the result of your \"train_classifier\" fucntion are all above 79%.\n",
        "\n",
        "    HINT: Perform a gridsearch to find optimized parameters to improve your classifier.\n",
        "\n",
        "    :return: accuracy, precision, recall, f1, vectorizer\n",
        "    \"\"\"\n",
        "    #clf = MultinomialNB()\n",
        "\n",
        "    #Optimized classifier\n",
        "    clf = MultinomialNB(alpha=0.1, class_prior= [0.1, 0.9], fit_prior=False)\n",
        "\n",
        "    params = {'alpha': [0.1,1], 'fit_prior': [False, True], 'class_prior':[[0.1,0.9],[0.6,0.4],[0.9,0.1]]}\n",
        "    accuracy, precision, recall, f1, v = train_classifier(df, clf, do_gridsearch=False, parameters=params)\n",
        "    print(accuracy, precision, recall, f1)\n",
        "    pred = clf.predict(v.transform([\"Who is Ghandi?\"]))\n",
        "    print(pred)\n",
        "    return accuracy, precision, recall, f1, v\n",
        "\n",
        "if not grading:\n",
        "    q = load_questions()\n",
        "    df = build_dataframe_q(q)\n",
        "    accuracy, precision, recall, f1, v = build_mnb(df)\n",
        "    print(accuracy, precision, recall, f1)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9029126213592233 0.9081634179544333 0.9029126213592233 0.9024146427833682\n",
            "['HUMAN']\n",
            "0.9029126213592233 0.9081634179544333 0.9029126213592233 0.9024146427833682\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QZRCJljUOkK",
        "colab_type": "code",
        "outputId": "d890e383-3666-432b-fbf9-99043c3ebbff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "def build_svc(df):\n",
        "    \"\"\"\n",
        "    Task 2c)\n",
        "\n",
        "    https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html  Train an SVC on the provided data.\n",
        "    Accuracy, precision, recall and f1-measure should all be above 96%.\n",
        "\n",
        "    Optimize the classifier parameters so the result of your \"train_classifier\" fucntion are all above 96%.\n",
        "\n",
        "    HINT: Perform a gridsearch to find optimized parameters to improve your classifier.\n",
        "\n",
        "    :return: accuracy, precision, recall, f1, vectorizer\n",
        "    \"\"\"\n",
        "    #clf = SVC()\n",
        "\n",
        "    #Optimized Classifier\n",
        "    clf = SVC(C=1, kernel='linear')\n",
        "\n",
        "    params = [{'C': [1,100, 1000], 'kernel':['linear']},{'C': [1,100, 1000], 'kernel':['rbf'], 'gamma': [0.1,0.2,0.3,0.4,0.5,0.6,0.7]}]\n",
        "  \n",
        "\n",
        "    accuracy, precision, recall, f1, v = train_classifier(df, clf, do_gridsearch=False, parameters= params)\n",
        "    print(accuracy, precision, recall, f1)\n",
        "    pred = clf.predict(v.transform([\"Who is Ghandi?\"]))\n",
        "    print(pred)\n",
        "    return accuracy, precision, recall, f1, v\n",
        "\n",
        "if not grading:\n",
        "    q = load_questions()\n",
        "    df = build_dataframe_q(q)\n",
        "    accuracy, precision, recall, f1, v = build_svc(df)\n",
        "    print(accuracy, precision, recall, f1)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9805825242718447 0.9805825242718447 0.9805825242718447 0.9805825242718447\n",
            "['HUMAN']\n",
            "0.9805825242718447 0.9805825242718447 0.9805825242718447 0.9805825242718447\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}